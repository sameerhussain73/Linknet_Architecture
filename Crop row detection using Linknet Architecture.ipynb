{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7iwp4LSnBPD",
    "outputId": "dda3fdd8-a3ab-4d2e-cb25-b7b7eb3c9ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5uz5esVg7exM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jD88-ZCH7gbE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\13177\\Downloads\\final_crop\\train and test ids.csv\", dtype = 'str')\n",
    "image_dir = r\"C:\\Users\\13177\\Downloads\\final_crop\\Images\\Images\"\n",
    "label_dir = r\"C:\\Users\\13177\\Downloads\\final_crop\\train_labels\\train_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p8KD2fH47pbo"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(ids, image_dir):\n",
    "    images = []\n",
    "\n",
    "    for img_id in ids:\n",
    "        img_id = int(img_id)  # Convert the ID to an integer to remove any decimal points\n",
    "        img_path = os.path.join(image_dir, f'crop_row_{img_id:03d}.jpg')  # Use string formatting to pad with zeros\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img) / 255.0  # Normalize pixel values between 0 and 1\n",
    "        images.append(img_array)\n",
    "\n",
    "    return np.array(images)\n",
    "# Load and preprocess the train and test images\n",
    "train_images = load_and_preprocess_images(df['train_ids'].dropna().values, image_dir)\n",
    "test_images = load_and_preprocess_images(df['test_ids'].dropna().values, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zZyE4g5B73Ln"
   },
   "outputs": [],
   "source": [
    "# Function to load the train labels\n",
    "def load_train_labels(ids, label_dir):\n",
    "    labels = []\n",
    "\n",
    "    for img_id in ids:\n",
    "        img_id = int(img_id) \n",
    "        label_path = os.path.join(label_dir, f'crop_row_{img_id:03d}.npy')\n",
    "        label = np.load(label_path)\n",
    "        # Consider only one channel\n",
    "        label = label[:, :, 0]\n",
    "        # Normalize between 0 and 1\n",
    "        label = label / 255.0\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(labels)\n",
    "\n",
    "# Load the train labels\n",
    "train_labels = load_train_labels(df['train_ids'].dropna().values, label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RbeQC0nG73I_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def encoder_block(input_tensor, filters):\n",
    "    x = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, filters):\n",
    "    x = layers.Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "    x = layers.concatenate([x, concat_tensor])\n",
    "    x = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def linknet(input_shape=(240, 320, 3)):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    e1 = encoder_block(inputs, 64)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(e1)\n",
    "\n",
    "    e2 = encoder_block(p1, 128)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(e2)\n",
    "\n",
    "    e3 = encoder_block(p2, 256)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(e3)\n",
    "\n",
    "    e4 = encoder_block(p3, 512)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(e4)\n",
    "\n",
    "    # Bridge\n",
    "    bridge = encoder_block(p4, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    d4 = decoder_block(bridge, e4, 512)\n",
    "    d3 = decoder_block(d4, e3, 256)\n",
    "    d2 = decoder_block(d3, e2, 128)\n",
    "    d1 = decoder_block(d2, e1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(d1)\n",
    "\n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sR1XElBdDq98"
   },
   "outputs": [],
   "source": [
    "train_labels = np.reshape(train_labels,(210,240,320,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T_GgbKoVD0kv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 240, 320, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8Scd74_l73Cc"
   },
   "outputs": [],
   "source": [
    "model = linknet()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z56p9W5p79Ou"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 38s 443ms/step - loss: 0.6345 - accuracy: 0.7238 - val_loss: 2.7802 - val_accuracy: 0.3513\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 15s 369ms/step - loss: 0.4478 - accuracy: 0.9020 - val_loss: 0.4356 - val_accuracy: 0.9319\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 23s 546ms/step - loss: 0.3230 - accuracy: 0.9280 - val_loss: 1.4981 - val_accuracy: 0.1360\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 23s 548ms/step - loss: 0.2464 - accuracy: 0.9325 - val_loss: 3.7689 - val_accuracy: 0.0862\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 23s 547ms/step - loss: 0.2100 - accuracy: 0.9344 - val_loss: 0.3811 - val_accuracy: 0.9291\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 23s 552ms/step - loss: 0.1923 - accuracy: 0.9350 - val_loss: 1.2209 - val_accuracy: 0.2138\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 26s 613ms/step - loss: 0.1829 - accuracy: 0.9352 - val_loss: 0.8971 - val_accuracy: 0.2727\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 24s 563ms/step - loss: 0.1809 - accuracy: 0.9350 - val_loss: 0.7105 - val_accuracy: 0.4877\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 25s 599ms/step - loss: 0.1738 - accuracy: 0.9352 - val_loss: 0.3825 - val_accuracy: 0.8880\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 22s 524ms/step - loss: 0.1718 - accuracy: 0.9353 - val_loss: 1.0678 - val_accuracy: 0.3905\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 23s 548ms/step - loss: 0.1691 - accuracy: 0.9360 - val_loss: 0.3536 - val_accuracy: 0.9078\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 23s 549ms/step - loss: 0.1684 - accuracy: 0.9357 - val_loss: 0.2363 - val_accuracy: 0.9300\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 23s 547ms/step - loss: 0.1663 - accuracy: 0.9364 - val_loss: 0.1864 - val_accuracy: 0.9331\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 23s 545ms/step - loss: 0.1651 - accuracy: 0.9362 - val_loss: 0.1865 - val_accuracy: 0.9325\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 23s 547ms/step - loss: 0.1635 - accuracy: 0.9366 - val_loss: 0.1790 - val_accuracy: 0.9341\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 23s 549ms/step - loss: 0.1626 - accuracy: 0.9367 - val_loss: 0.2006 - val_accuracy: 0.9286\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 23s 548ms/step - loss: 0.1636 - accuracy: 0.9368 - val_loss: 0.1806 - val_accuracy: 0.9346\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 28s 664ms/step - loss: 0.1614 - accuracy: 0.9375 - val_loss: 0.2137 - val_accuracy: 0.9308\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 24s 578ms/step - loss: 0.1588 - accuracy: 0.9374 - val_loss: 0.1815 - val_accuracy: 0.9335\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 24s 572ms/step - loss: 0.1588 - accuracy: 0.9374 - val_loss: 0.1771 - val_accuracy: 0.9337\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 24s 570ms/step - loss: 0.1567 - accuracy: 0.9382 - val_loss: 0.2040 - val_accuracy: 0.9234\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.1552 - accuracy: 0.9385 - val_loss: 0.1829 - val_accuracy: 0.9330\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 24s 572ms/step - loss: 0.1545 - accuracy: 0.9383 - val_loss: 0.1738 - val_accuracy: 0.9351\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.1528 - accuracy: 0.9390 - val_loss: 0.1762 - val_accuracy: 0.9339\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 24s 570ms/step - loss: 0.1526 - accuracy: 0.9390 - val_loss: 0.2123 - val_accuracy: 0.9317\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 24s 571ms/step - loss: 0.1530 - accuracy: 0.9385 - val_loss: 0.1810 - val_accuracy: 0.9338\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 23s 546ms/step - loss: 0.1484 - accuracy: 0.9400 - val_loss: 0.1740 - val_accuracy: 0.9349\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 20s 466ms/step - loss: 0.1489 - accuracy: 0.9401 - val_loss: 0.1945 - val_accuracy: 0.9317\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 23s 543ms/step - loss: 0.1488 - accuracy: 0.9395 - val_loss: 0.1785 - val_accuracy: 0.9357\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 23s 552ms/step - loss: 0.1440 - accuracy: 0.9411 - val_loss: 0.1719 - val_accuracy: 0.9334\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 25s 605ms/step - loss: 0.1480 - accuracy: 0.9400 - val_loss: 0.1862 - val_accuracy: 0.9298\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 22s 522ms/step - loss: 0.1454 - accuracy: 0.9402 - val_loss: 0.1747 - val_accuracy: 0.9332\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 25s 589ms/step - loss: 0.1406 - accuracy: 0.9420 - val_loss: 0.1790 - val_accuracy: 0.9329\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 25s 589ms/step - loss: 0.1404 - accuracy: 0.9417 - val_loss: 0.1863 - val_accuracy: 0.9324\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 22s 521ms/step - loss: 0.1424 - accuracy: 0.9413 - val_loss: 0.1788 - val_accuracy: 0.9352\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 22s 518ms/step - loss: 0.1359 - accuracy: 0.9432 - val_loss: 0.1946 - val_accuracy: 0.9323\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 23s 562ms/step - loss: 0.1344 - accuracy: 0.9437 - val_loss: 0.1825 - val_accuracy: 0.9332\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 24s 566ms/step - loss: 0.1288 - accuracy: 0.9454 - val_loss: 0.1847 - val_accuracy: 0.9323\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 21s 491ms/step - loss: 0.1306 - accuracy: 0.9449 - val_loss: 0.1870 - val_accuracy: 0.9327\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 21s 498ms/step - loss: 0.1254 - accuracy: 0.9464 - val_loss: 0.1776 - val_accuracy: 0.9337\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 22s 532ms/step - loss: 0.1248 - accuracy: 0.9465 - val_loss: 0.2023 - val_accuracy: 0.9310\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 22s 532ms/step - loss: 0.1219 - accuracy: 0.9474 - val_loss: 0.1935 - val_accuracy: 0.9316\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 22s 532ms/step - loss: 0.1191 - accuracy: 0.9484 - val_loss: 0.1913 - val_accuracy: 0.9307\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 22s 529ms/step - loss: 0.1175 - accuracy: 0.9489 - val_loss: 0.2119 - val_accuracy: 0.9289\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 22s 527ms/step - loss: 0.1134 - accuracy: 0.9506 - val_loss: 0.1936 - val_accuracy: 0.9287\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 22s 524ms/step - loss: 0.1091 - accuracy: 0.9528 - val_loss: 0.2277 - val_accuracy: 0.9268\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 22s 525ms/step - loss: 0.1101 - accuracy: 0.9521 - val_loss: 0.2093 - val_accuracy: 0.9287\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 19s 453ms/step - loss: 0.1071 - accuracy: 0.9534 - val_loss: 0.2094 - val_accuracy: 0.9287\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 23s 550ms/step - loss: 0.1044 - accuracy: 0.9542 - val_loss: 0.2144 - val_accuracy: 0.9282\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 23s 552ms/step - loss: 0.1001 - accuracy: 0.9563 - val_loss: 0.2077 - val_accuracy: 0.9268\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 25s 608ms/step - loss: 0.0990 - accuracy: 0.9565 - val_loss: 0.2242 - val_accuracy: 0.9278\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 23s 545ms/step - loss: 0.0935 - accuracy: 0.9592 - val_loss: 0.2229 - val_accuracy: 0.9303\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 23s 546ms/step - loss: 0.0926 - accuracy: 0.9598 - val_loss: 0.2092 - val_accuracy: 0.9269\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 20s 481ms/step - loss: 0.0910 - accuracy: 0.9603 - val_loss: 0.2424 - val_accuracy: 0.9257\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 21s 501ms/step - loss: 0.0878 - accuracy: 0.9616 - val_loss: 0.2218 - val_accuracy: 0.9256\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 22s 532ms/step - loss: 0.0867 - accuracy: 0.9620 - val_loss: 0.2436 - val_accuracy: 0.9269\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 22s 537ms/step - loss: 0.0873 - accuracy: 0.9620 - val_loss: 0.2538 - val_accuracy: 0.9262\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 23s 538ms/step - loss: 0.0821 - accuracy: 0.9642 - val_loss: 0.2401 - val_accuracy: 0.9283\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 25s 599ms/step - loss: 0.0831 - accuracy: 0.9638 - val_loss: 0.2590 - val_accuracy: 0.9227\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 26s 615ms/step - loss: 0.0800 - accuracy: 0.9651 - val_loss: 0.2459 - val_accuracy: 0.9265\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 20s 471ms/step - loss: 0.0795 - accuracy: 0.9651 - val_loss: 0.2533 - val_accuracy: 0.9260\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 20s 488ms/step - loss: 0.0763 - accuracy: 0.9669 - val_loss: 0.2648 - val_accuracy: 0.9248\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 23s 539ms/step - loss: 0.0748 - accuracy: 0.9674 - val_loss: 0.2534 - val_accuracy: 0.9255\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 20s 486ms/step - loss: 0.0779 - accuracy: 0.9659 - val_loss: 0.2494 - val_accuracy: 0.9244\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 22s 525ms/step - loss: 0.0746 - accuracy: 0.9674 - val_loss: 0.2673 - val_accuracy: 0.9255\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 19s 461ms/step - loss: 0.0710 - accuracy: 0.9691 - val_loss: 0.2563 - val_accuracy: 0.9251\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 20s 481ms/step - loss: 0.0697 - accuracy: 0.9696 - val_loss: 0.2796 - val_accuracy: 0.9226\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0676 - accuracy: 0.9706 - val_loss: 0.2748 - val_accuracy: 0.9222\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0665 - accuracy: 0.9711 - val_loss: 0.2799 - val_accuracy: 0.9231\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 20s 485ms/step - loss: 0.0704 - accuracy: 0.9692 - val_loss: 0.2758 - val_accuracy: 0.9184\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 20s 482ms/step - loss: 0.0647 - accuracy: 0.9719 - val_loss: 0.2865 - val_accuracy: 0.9246\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0614 - accuracy: 0.9733 - val_loss: 0.2802 - val_accuracy: 0.9259\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 20s 482ms/step - loss: 0.0616 - accuracy: 0.9731 - val_loss: 0.2992 - val_accuracy: 0.9247\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 20s 481ms/step - loss: 0.0625 - accuracy: 0.9727 - val_loss: 0.2947 - val_accuracy: 0.9229\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0620 - accuracy: 0.9730 - val_loss: 0.2921 - val_accuracy: 0.9236\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 20s 482ms/step - loss: 0.0603 - accuracy: 0.9737 - val_loss: 0.3033 - val_accuracy: 0.9236\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0604 - accuracy: 0.9737 - val_loss: 0.2983 - val_accuracy: 0.9231\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0604 - accuracy: 0.9736 - val_loss: 0.2926 - val_accuracy: 0.9232\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 20s 486ms/step - loss: 0.0553 - accuracy: 0.9759 - val_loss: 0.2975 - val_accuracy: 0.9247\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 20s 487ms/step - loss: 0.0555 - accuracy: 0.9758 - val_loss: 0.3142 - val_accuracy: 0.9228\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 20s 485ms/step - loss: 0.0583 - accuracy: 0.9747 - val_loss: 0.2793 - val_accuracy: 0.9243\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 20s 486ms/step - loss: 0.0592 - accuracy: 0.9741 - val_loss: 0.3110 - val_accuracy: 0.9256\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 20s 484ms/step - loss: 0.0530 - accuracy: 0.9769 - val_loss: 0.3281 - val_accuracy: 0.9248\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 20s 483ms/step - loss: 0.0601 - accuracy: 0.9745 - val_loss: 0.2916 - val_accuracy: 0.9225\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 22s 537ms/step - loss: 0.0619 - accuracy: 0.9732 - val_loss: 0.3056 - val_accuracy: 0.9184\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 21s 504ms/step - loss: 0.0568 - accuracy: 0.9755 - val_loss: 0.3094 - val_accuracy: 0.9237\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 20s 486ms/step - loss: 0.0512 - accuracy: 0.9777 - val_loss: 0.3276 - val_accuracy: 0.9230\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 21s 496ms/step - loss: 0.0507 - accuracy: 0.9779 - val_loss: 0.3147 - val_accuracy: 0.9230\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 21s 496ms/step - loss: 0.0497 - accuracy: 0.9784 - val_loss: 0.3274 - val_accuracy: 0.9231\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 22s 514ms/step - loss: 0.0481 - accuracy: 0.9789 - val_loss: 0.3348 - val_accuracy: 0.9239\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 22s 523ms/step - loss: 0.0462 - accuracy: 0.9799 - val_loss: 0.3423 - val_accuracy: 0.9235\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 21s 512ms/step - loss: 0.0470 - accuracy: 0.9794 - val_loss: 0.3316 - val_accuracy: 0.9212\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 21s 512ms/step - loss: 0.0456 - accuracy: 0.9801 - val_loss: 0.3449 - val_accuracy: 0.9232\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 19s 449ms/step - loss: 0.0454 - accuracy: 0.9802 - val_loss: 0.3425 - val_accuracy: 0.9222\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 19s 451ms/step - loss: 0.0475 - accuracy: 0.9793 - val_loss: 0.3516 - val_accuracy: 0.9221\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 20s 471ms/step - loss: 0.0450 - accuracy: 0.9805 - val_loss: 0.3449 - val_accuracy: 0.9229\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 20s 470ms/step - loss: 0.0431 - accuracy: 0.9811 - val_loss: 0.3515 - val_accuracy: 0.9241\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 18s 426ms/step - loss: 0.0422 - accuracy: 0.9816 - val_loss: 0.3483 - val_accuracy: 0.9229\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 16s 388ms/step - loss: 0.0421 - accuracy: 0.9816 - val_loss: 0.3575 - val_accuracy: 0.9213\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 17s 418ms/step - loss: 0.0427 - accuracy: 0.9813 - val_loss: 0.3520 - val_accuracy: 0.9234\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size=4, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovm309qXhpWr"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss','validation loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training accuracy','validation accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZi9urCg8oDA"
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    '''\n",
    "    mask: numpy array binary mask \n",
    "    255 - mask \n",
    "    0 - background\n",
    "    Returns encoded run length \n",
    "    '''\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GdHSWNa8_A4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    #img = img.resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize pixel values between 0 and 1\n",
    "    return np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "\n",
    "# Load the image you want to predict\n",
    "image_path = \"/content/drive/MyDrive/project/Images/Images/crop_row_227.jpg\"\n",
    "label_path = \"/content/drive/MyDrive/project/train_labels/train_labels/crop_row_227.npy\"\n",
    "image = load_and_preprocess_image(image_path)\n",
    "\n",
    "# Predict the segmentation using the trained model\n",
    "segmentation = model.predict(image)\n",
    "\n",
    "# Remove the batch dimension and convert the segmentation to binary values (0 or 255)\n",
    "segmentation = (segmentation.squeeze() * 255).astype(np.uint8)\n",
    "threshold = 0.20\n",
    "output = (segmentation/np.max(segmentation) > threshold).astype(np.uint8)\n",
    "\n",
    "\n",
    "label = np.load(label_path)[:, :, 0] // 255\n",
    "\n",
    "# Plot the input image, label, and predicted mask\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "axs[0].imshow(image.squeeze())\n",
    "axs[0].set_title('Input Image')\n",
    "axs[1].imshow(label, cmap='gray')\n",
    "axs[1].set_title('Ground Truth Label')\n",
    "axs[2].imshow(output, cmap='gray')\n",
    "axs[2].set_title('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eviONXsVhAtf"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRHDyvIuE5ZL"
   },
   "outputs": [],
   "source": [
    "def IOU(output,label):\n",
    "  overlap = label*output\n",
    "  union = label+output\n",
    "  IOU = overlap.sum()/float(union.sum())\n",
    "  return(IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrjqBKVLGjCx"
   },
   "outputs": [],
   "source": [
    "train_ids = df['train_ids'].dropna().values\n",
    "test_ids = df['test_ids'].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4CrQ_Zt-BxE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img_array = np.array(img) / 255.0  # Normalize pixel values between 0 and 1\n",
    "    return np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "\n",
    "# Function to load the train labels\n",
    "def load_and_preprocess_label(label_path):\n",
    "    label = np.load(label_path)\n",
    "    # Consider only one channel\n",
    "    label = label[:, :, 0]\n",
    "    # Normalize between 0 and 1\n",
    "    label = label / 255.0\n",
    "    return (label)\n",
    "  \n",
    "IOU_total = []\n",
    "for img in train_ids:\n",
    "  img_id = int(img)\n",
    "  # Load the image you want to predict\n",
    "  image_path = os.path.join(image_dir, f'crop_row_{img_id:03d}.jpg')\n",
    "  label_path = os.path.join(label_dir, f'crop_row_{img_id:03d}.npy')\n",
    "  image = load_and_preprocess_image(image_path)\n",
    "  label = load_and_preprocess_label(label_path)\n",
    "\n",
    "  # Predict the segmentation using the trained model\n",
    "  segmentation = model.predict(image)\n",
    "\n",
    "  # Remove the batch dimension and convert the segmentation to binary values (0 or 255)\n",
    "  segmentation = (segmentation.squeeze() * 255).astype(np.uint8)\n",
    "\n",
    "  threshold = 0.5\n",
    "  output = (segmentation/np.max(segmentation) > threshold).astype(np.uint8)\n",
    "  IOU_total.append(IOU(output,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIcZK-Qf-63b"
   },
   "outputs": [],
   "source": [
    "sum(IOU_total)/len(IOU_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRK4RXRYORdM"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------Testing------------------------------------------#\n",
    "rle_enconde_list = []\n",
    "for img in test_ids:\n",
    "  img_id = int(img)\n",
    "  # Load the image you want to predict\n",
    "  image_path = os.path.join(image_dir, f'crop_row_{img_id:03d}.jpg')\n",
    "  image = load_and_preprocess_image(image_path)\n",
    "\n",
    "  # Predict the segmentation using the trained model\n",
    "  segmentation = model.predict(image)\n",
    "\n",
    "  # Remove the batch dimension and convert the segmentation to binary values (0 or 255)\n",
    "  segmentation = (segmentation.squeeze() * 255).astype(np.uint8)\n",
    "\n",
    "  threshold = 0.5\n",
    "  output = (segmentation/np.max(segmentation) > threshold).astype(np.uint8)\n",
    "  rle_enconde_list.append(rle_encode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdVMpKpnkkjS"
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "d['ids'] = list(test_ids)\n",
    "d['labels'] = rle_enconde_list\n",
    "df_test = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0EQwu0gk69o"
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcax4y86k7ty"
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('Linknet-batch_size=4, epochs=100, validation_split=0.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65aYXcPYk9JM"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('Linknet-batch_size=4, epochs=100, validation_split=0.2.csv') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
